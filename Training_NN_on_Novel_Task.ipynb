{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#  Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_otiLR6kgUDc",
        "outputId": "b7ddb512-58aa-4188-ecd5-10c15f2e7926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium"
      ],
      "metadata": {
        "id": "6k4JywPCA8Rz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c67b3ab-8c55-4875-d162-bd62b5c01c2d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gymnasium\n",
            "  Downloading gymnasium-1.0.0-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.26.4)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.12.2)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl.metadata (558 bytes)\n",
            "Downloading gymnasium-1.0.0-py3-none-any.whl (958 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m958.1/958.1 kB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Installing collected packages: farama-notifications, gymnasium\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-1.0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "from itertools import count\n",
        "from collections import deque\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "batch_size = 32\n",
        "learning_rate = 1e-2\n",
        "\n",
        "input_size = 8\n",
        "\n",
        "hidden_size = 128\n",
        "num_classes= 2\n",
        "num_layers = 1\n",
        "\n",
        "\n",
        "\n",
        "gamma=0.99\n",
        "\n",
        "\n",
        "log_interval=100\n",
        "\n",
        "scores, running_scores= [],  []\n",
        "\n",
        "\n",
        "class Policy(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Policy, self).__init__()\n",
        "        self.affine1 = nn.Linear(input_size, hidden_size)\n",
        "        self.dropout = nn.Dropout(p=0.2)\n",
        "        self.affine2 = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "        self.saved_log_probs = []\n",
        "        self.rewards = []\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.affine1(x)\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(x)\n",
        "        action_scores = self.affine2(x)\n",
        "        return F.softmax(action_scores, dim=1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "env = gym.make(\"two-step-task-novel-v0\", render_mode='human')\n",
        "\n",
        "\n",
        "policy = Policy()\n",
        "#comment optimizer if continuing training for a saved checkpoint\n",
        "optimizer = optim.Adam(policy.parameters(), lr=learning_rate)\n",
        "\n"
      ],
      "metadata": {
        "id": "Q1DsJ6NptJxd"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the checkpoint from Google Drive\n",
        "path = \"/content/drive/MyDrive/Models/folder_name/\"\n",
        "\n",
        "file_name_trained=\"best_checkpoint_atThreshold_2562.pth\"\n",
        "checkpoint_tarined = torch.load(f\"{path}{file_name_trained}\", weights_only=False)\n",
        "\n",
        "# Load the model and optimizer state\n",
        "optimizer = optim.Adam(policy.parameters())  # Recreate the optimizer with the same settings as before\n",
        "policy.load_state_dict(checkpoint_tarined['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint_tarined['optimizer_state_dict'])\n",
        "policy.train()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4Keu1s2Nqff",
        "outputId": "c6e8297b-ae1b-4be0-ffc5-6c9db74ec536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Policy(\n",
              "  (affine1): Linear(in_features=8, out_features=128, bias=True)\n",
              "  (dropout): Dropout(p=0.2, inplace=False)\n",
              "  (affine2): Linear(in_features=128, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Epsilon value to avoid division by zero during normalization\n",
        "eps = np.finfo(np.float32).eps.item()\n",
        "\n",
        "# Function to select an action based on the current state\n",
        "def select_action(state):\n",
        "    # Convert the state to a PyTorch tensor, add a batch dimension, and move to the device (CPU or GPU)\n",
        "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "\n",
        "    # Get the action probabilities from the policy model\n",
        "    probs = policy(state)\n",
        "\n",
        "    # Create a categorical distribution using the action probabilities\n",
        "    action_dist = Categorical(probs)\n",
        "\n",
        "    # Sample an action from the distribution\n",
        "    action = action_dist.sample()\n",
        "\n",
        "    # Store the log probability of the taken action for later use in the policy loss computation\n",
        "    policy.saved_log_probs.append(action_dist.log_prob(action))\n",
        "\n",
        "    # Return the action as a Python integer (scalar)\n",
        "    return action.item()\n",
        "\n",
        "# Function to finish an episode, calculate the policy loss, and perform backpropagation\n",
        "def finish_episode():\n",
        "    # Initialize the return value (R) and lists to store the policy loss and the computed returns\n",
        "    R = 0\n",
        "    policy_loss = []\n",
        "    returns = deque()\n",
        "\n",
        "    # Loop through the rewards in reverse order to calculate the discounted returns (R)\n",
        "    for r in policy.rewards[::-1]:\n",
        "        R = r + gamma * R  # Apply the discount factor (gamma)\n",
        "        returns.appendleft(R)  # Store the return value at the beginning of the deque\n",
        "\n",
        "    # Convert returns to a tensor\n",
        "    returns = torch.tensor(returns)\n",
        "\n",
        "    # Normalize the returns (zero mean, unit variance) to stabilize training\n",
        "    returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "\n",
        "    # Compute the policy loss using the log probability of each action and the computed return\n",
        "    for log_prob, R in zip(policy.saved_log_probs, returns):\n",
        "        policy_loss.append(-log_prob * R)  # Negative log probability is used for gradient ascent\n",
        "\n",
        "    # Zero out gradients before backpropagation\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Concatenate the policy losses and sum them up\n",
        "    policy_loss = torch.cat(policy_loss).sum()\n",
        "\n",
        "    # Perform backpropagation on the policy loss\n",
        "    policy_loss.backward()\n",
        "\n",
        "    # Update the model parameters using the optimizer\n",
        "    optimizer.step()\n",
        "\n",
        "    # Clear the stored rewards and log probabilities to start fresh for the next episode\n",
        "    del policy.rewards[:]\n",
        "    del policy.saved_log_probs[:]\n",
        "\n",
        "    return\n",
        "\n",
        "# Main function to run the reinforcement learning training loop\n",
        "def main():\n",
        "    # Define the folder path where models will be saved\n",
        "    folder_path = '/content/drive/MyDrive/Models/folder_name/'\n",
        "\n",
        "    # Set the threshold for the running reward to consider the task solved\n",
        "    threshold = 700\n",
        "    running_reward = 10  # Initialize the running reward\n",
        "\n",
        "    # Loop over episodes\n",
        "    for i_episode in range(1001):\n",
        "        # Reset the environment and get the initial state\n",
        "        state = env.reset()[0]\n",
        "        state = np.array(state)\n",
        "        ep_reward = 0  # Initialize the episode reward\n",
        "\n",
        "        # Loop over time steps within an episode\n",
        "        for t in range(1, 10000):  # Limit the maximum number of time steps to avoid infinite loops\n",
        "\n",
        "            # Select an action based on the current state\n",
        "            action = select_action(state)\n",
        "\n",
        "            # Take the action and get the next state, reward, and whether the episode is done\n",
        "            state, reward, done, _, info = env.step(action)\n",
        "\n",
        "            # Add the reward for the current time step to the episode reward\n",
        "            ep_reward += reward\n",
        "\n",
        "            # Store the reward for this time step for later use in the policy loss calculation\n",
        "            policy.rewards.append(reward)\n",
        "\n",
        "\n",
        "            # If the episode is done, break out of the loop\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        # Append the episode's total reward to the scores list\n",
        "        scores.append(ep_reward)\n",
        "        if i_episode ==1:\n",
        "              return\n",
        "        # Update the running average of the episode rewards using an exponential moving average\n",
        "        running_reward = 0.05 * ep_reward + (1 - 0.05) * running_reward\n",
        "\n",
        "        # Call finish_episode to calculate the policy loss and update the model\n",
        "        finish_episode()\n",
        "\n",
        "        # Log the episode results every log_interval episodes\n",
        "        if i_episode % log_interval == 0:\n",
        "            print(f'Episode {i_episode} Last reward: {ep_reward} Average reward: {running_reward}')\n",
        "\n",
        "        # If the running reward exceeds the threshold, save the model checkpoint\n",
        "        if running_reward >= threshold:\n",
        "            threshold = running_reward  # Update the threshold to the new running reward\n",
        "            print(f\"Solved! eps {i_episode} Running reward is now {running_reward}\")\n",
        "\n",
        "            # Create a checkpoint dictionary to save the model and optimizer states\n",
        "            checkpoint = {\n",
        "                'model_state_dict': policy.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'score': ep_reward\n",
        "            }\n",
        "\n",
        "            # Save the checkpoint to a file with details about the threshold and reward\n",
        "            torch.save(checkpoint, f'best_checkpoint_atThreshold{threshold}_runing{running_reward}_eps{i_episode}_score_{ep_reward}.pth')\n",
        "            print(f\"New best running score: {running_reward} . Checkpoint saved!\")\n",
        "\n",
        "            # Move the checkpoint file to the specified Google Drive folder\n",
        "            !mv /content/best*.pth /content/drive/MyDrive/Models/folder_name/\n",
        "\n",
        "        # If it's the last episode (episode 1000), save the final checkpoint\n",
        "        if i_episode == 1000:\n",
        "            print(f\"last episode! Running reward is now {running_reward}\")\n",
        "\n",
        "            # Create a final checkpoint dictionary\n",
        "            checkpoint = {\n",
        "                'model_state_dict': policy.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'score': ep_reward\n",
        "            }\n",
        "\n",
        "            # Save the final checkpoint\n",
        "            torch.save(checkpoint, f'best_checkpoint_attheEnd_score_{ep_reward}_runing{running_reward}.pth')\n",
        "            print(f\"last best score: {ep_reward} . Checkpoint saved!\")\n",
        "\n",
        "            # Move the final checkpoint to the Google Drive folder\n",
        "            !mv /content/best*.pth /content/drive/MyDrive/Models/folder_name/\n",
        "            return\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rtBcpEFO45g",
        "outputId": "feeaee03-17f5-487a-e71f-3d989fe8ed55"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Blu   Gre \n",
            "      \n",
            "  Age   \n",
            "\n",
            "Episode 0 Last reward: 2259 Average reward: 122.45\n",
            "Ora   Tur \n",
            "      \n",
            "  Age   \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "snTpKC6fH1hx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import gymnasium as gym  # Gymnasium (new version of Gym)\n",
        "from enum import Enum\n",
        "import random\n",
        "import numpy as np\n",
        "from scipy.stats import norm\n",
        "\n",
        "from os import truncate\n",
        "\n",
        "# Enum defining the possible actions the agent can take in the environment: LEFT or RIGHT\n",
        "class AgentAction(Enum):\n",
        "    LEFT = 0\n",
        "    RIGHT = 1\n",
        "\n",
        "    # Method to get the integer value of the action (0 or 1)\n",
        "    def get_value(self):\n",
        "        return self.value\n",
        "\n",
        "# Enum defining various grid tiles in the environment (e.g., agent, spaceships, aliens)\n",
        "class GridTile(Enum):\n",
        "    Agent = 0\n",
        "    Green_Spaceship = 1\n",
        "    Blue_Spaceship = 2\n",
        "    Turquoise_Spaceship = 3\n",
        "    Orange_Spaceship = 4\n",
        "    Red_Alien = 5\n",
        "    Purple_Alien = 6\n",
        "\n",
        "    # Method to return the first 3 letters of the tile name for printing\n",
        "    def __str__(self):\n",
        "        return self.name[:3]\n",
        "\n",
        "    # Method to get the integer value associated with the grid tile\n",
        "    def get_value(self):\n",
        "        return self.value\n",
        "\n",
        "# Class representing the Two-Step Task, a custom task environment for reinforcement learning\n",
        "class Two_Step_Task:\n",
        "\n",
        "    # Initialize the grid size and task setup\n",
        "    def __init__(self, grid_rows=3, grid_cols=3, fps=1):\n",
        "        self.grid_rows = grid_rows\n",
        "        self.grid_cols = grid_cols\n",
        "        self.reset()\n",
        "\n",
        "    # Reset the environment to its initial state\n",
        "    def reset(self, seed=None):\n",
        "        random.seed(seed)  # Set the seed for reproducibility\n",
        "        self.state = [0, 0, 0, 0]  # Initial state undefined\n",
        "        self.assign_stimulus()  # Assign stimulus for the left and right actions and sets state\n",
        "\n",
        "        # Initialize positions for rendering the agent and the stimuli\n",
        "        self.left_stim_pos = [0, 0]\n",
        "        self.right_stim_pos = [0, 2]\n",
        "        self.agent_pos = [2, 1]\n",
        "\n",
        "        self.generate_rewards()  # Generate reward probabilities for each action state pair\n",
        "        self.trial_num = 0  # Initialize trial number\n",
        "        self.winning_prob_matrix = []  # Reward probabilities matrix\n",
        "        self.stake = 1  # Initial stake (can be 1 or 5)\n",
        "        self.score = 0  # Initial score\n",
        "\n",
        "    # Reset for a new trial\n",
        "    def reset_trial(self):\n",
        "        self.state = [0, 0, 0, 0]  # Reset state to initial\n",
        "        self.assign_stimulus()  # Re-assign stimuli\n",
        "        self.left_stim_pos = [0, 0]\n",
        "        self.right_stim_pos = [0, 2]\n",
        "        self.agent_pos = [2, 1]\n",
        "\n",
        "    # Generate reward probabilities for each trial based on Gaussian noise\n",
        "    def generate_rewards(self):\n",
        "        self.bounds = [0, 9]  # Set reward bounds\n",
        "        self.sd = 2  # Standard deviation for noise\n",
        "        self.choices = 1  # Only one choice is made in this novel task\n",
        "        self.nrtrials = 200  # Number of trials\n",
        "\n",
        "        # Ensure the bounds are sorted\n",
        "        self.bounds = np.sort(self.bounds)\n",
        "\n",
        "        # Initialize rewards matrix\n",
        "        self.rewards = np.zeros((self.nrtrials, 2, self.choices))\n",
        "\n",
        "        # Randomly generate reward probabilities for the first trial\n",
        "        if np.random.rand() < 0.5:\n",
        "            if np.random.rand() < 0.5:\n",
        "                x = np.array([0, 4])\n",
        "            else:\n",
        "                x = np.array([4, 0])\n",
        "        else:\n",
        "            if np.random.rand() < 0.5:\n",
        "                x = np.array([[5, 9]])\n",
        "            else:\n",
        "                x = np.array([[9, 5]])\n",
        "\n",
        "        # Set the reward for the first trial\n",
        "        self.rewards[0, :, 0] = x\n",
        "\n",
        "        # Loop through each trial to update the rewards with Gaussian noise\n",
        "        for t in range(1, self.nrtrials):\n",
        "            for s in range(2):  # Two states (based on stimulus)\n",
        "                for a in range(self.choices):  # One choice per trial\n",
        "                    # Update reward with noise and ensure it stays within bounds\n",
        "                    self.rewards[t, s, a] = self.rewards[t - 1, s, a] + norm.rvs(scale=self.sd)\n",
        "                    self.rewards[t, s, a] = min(self.rewards[t, s, a], max(self.bounds[1] * 2 - self.rewards[t, s, a], self.bounds[0]))\n",
        "                    self.rewards[t, s, a] = max(self.rewards[t, s, a], min(self.bounds[0] * 2 - self.rewards[t, s, a], self.bounds[1]))\n",
        "\n",
        "        # Round rewards to 3 decimal places\n",
        "        self.rewards = np.round(self.rewards, 3)\n",
        "        return self.rewards\n",
        "\n",
        "    # Reset the stake randomly (either 1 or 5)\n",
        "    def reset_stake(self):\n",
        "        p = random.random()\n",
        "        if p <= 0.5:\n",
        "            self.stake = 1\n",
        "        else:\n",
        "            self.stake = 5\n",
        "\n",
        "    # Assign state and a stimulus (spaceship type) to the agent's environment\n",
        "    def assign_stimulus(self, stim_1=GridTile.Green_Spaceship, stim_2=GridTile.Blue_Spaceship,\n",
        "                        stim_3=GridTile.Turquoise_Spaceship, stim_4=GridTile.Orange_Spaceship):\n",
        "        p = random.random()\n",
        "        if p <= 0.5:\n",
        "            p = random.random()\n",
        "            if p <= 0.5:\n",
        "                self.left_stim = stim_1\n",
        "                self.right_stim = stim_2\n",
        "            else:\n",
        "                self.left_stim = stim_2\n",
        "                self.right_stim = stim_1\n",
        "            self.state = [1, 0, 0, 0]  # State for first stimulus pair\n",
        "        else:\n",
        "            p = random.random()\n",
        "            if p <= 0.5:\n",
        "                self.left_stim = stim_3\n",
        "                self.right_stim = stim_4\n",
        "            else:\n",
        "                self.left_stim = stim_4\n",
        "                self.right_stim = stim_3\n",
        "            self.state = [0, 1, 0, 0]  # State for second stimulus pair\n",
        "\n",
        "        self.state_part2 = [self.left_stim.get_value(), self.right_stim.get_value()]\n",
        "\n",
        "    # Assign a planet based on the spaceship type (Green, Blue, Turquoise, Orange)\n",
        "    def assign_planet(self, space_ship: GridTile):\n",
        "        p = random.random()\n",
        "        match space_ship:\n",
        "            case GridTile.Green_Spaceship:\n",
        "                self.state = [0, 0, 1, 0]  # Red planet\n",
        "                self.state_part2 = [GridTile.Red_Alien.get_value(), 0]\n",
        "            case GridTile.Blue_Spaceship:\n",
        "                self.state = [0, 0, 0, 1]  # Purple planet\n",
        "                self.state_part2 = [0, GridTile.Purple_Alien.get_value()]\n",
        "            case GridTile.Turquoise_Spaceship:\n",
        "                self.state = [0, 0, 1, 0]  # Red planet\n",
        "                self.state_part2 = [GridTile.Red_Alien.get_value(), 0]\n",
        "            case GridTile.Orange_Spaceship:\n",
        "                self.state = [0, 0, 0, 1]  # Purple planet\n",
        "                self.state_part2 = [0, GridTile.Purple_Alien.get_value()]\n",
        "\n",
        "    # Perform the agent's action (LEFT or RIGHT) and update the state\n",
        "    def perform_action(self, agent_action: AgentAction):\n",
        "        if agent_action == AgentAction.LEFT:\n",
        "            self.assign_planet(self.left_stim)\n",
        "\n",
        "        if agent_action == AgentAction.RIGHT:\n",
        "            self.assign_planet(self.right_stim)\n",
        "\n",
        "        # Determine the reward index based on the current state and action\n",
        "        reward_ind_state = self.state.index(1) - 2\n",
        "        reward_ind_action = 0\n",
        "        self.reward_prob = self.rewards[self.trial_num, reward_ind_state, reward_ind_action]\n",
        "        self.winning_prob_matrix = self.rewards[self.trial_num, :, :]\n",
        "        if self.trial_num <= 199:\n",
        "            self.trial_num += 1\n",
        "\n",
        "    # Render the environment's current state on the console\n",
        "    def render(self):\n",
        "        for r in range(self.grid_rows):\n",
        "            for c in range(self.grid_cols):\n",
        "                if [r, c] == self.agent_pos:\n",
        "                    print(GridTile.Agent, end=' ')\n",
        "                elif [r, c] == self.left_stim_pos:\n",
        "                    print(GridTile(self.left_stim), end=' ')\n",
        "                elif [r, c] == self.right_stim_pos:\n",
        "                    print(GridTile(self.right_stim), end=' ')\n",
        "                else:\n",
        "                    print(' ', end=' ')\n",
        "            print()\n",
        "        print()\n",
        "\n",
        "\n",
        "# Implementing the custom Gymnasium environment\n",
        "from gymnasium import spaces\n",
        "from gymnasium.envs.registration import register\n",
        "from gymnasium.utils.env_checker import check_env\n",
        "import numpy as np\n",
        "\n",
        "# Custom Gymnasium environment class\n",
        "class TwoStepTaskEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], 'render_fps': 1}  # Metadata for Gymnasium\n",
        "\n",
        "    # Initialize the environment\n",
        "    def __init__(self, grid_rows=3, grid_cols=3, render_mode=None):\n",
        "        self.grid_rows = grid_rows\n",
        "        self.grid_cols = grid_cols\n",
        "        self.render_mode = render_mode\n",
        "        self.input_size = 8  # Total size of the observation space\n",
        "\n",
        "        # Initialize the task agent\n",
        "        self.task_agent = Two_Step_Task(grid_rows=grid_rows, grid_cols=grid_cols, fps=self.metadata['render_fps'])\n",
        "\n",
        "        # Define action space (discrete action choices)\n",
        "        self.action_space = spaces.Discrete(len(AgentAction))\n",
        "\n",
        "        # Define observation space (size of the state vector)\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0,\n",
        "            high=4000,\n",
        "            shape=(self.input_size,),\n",
        "            dtype=np.int64\n",
        "        )\n",
        "\n",
        "    # Reset the environment and return the initial observation\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)  # Ensure reproducibility\n",
        "        self.task_agent.reset(seed=seed)\n",
        "        observation = self.task_agent.state + self.task_agent.state_part2 + [self.task_agent.stake] + [self.task_agent.score]\n",
        "\n",
        "        # Reshape observation and return\n",
        "        obs = np.array([observation], dtype=np.int64).reshape((self.input_size,))\n",
        "        info = {}  # Optional info dictionary\n",
        "\n",
        "        # Render environment if necessary\n",
        "        if self.render_mode == 'human':\n",
        "            self.render()\n",
        "\n",
        "        return obs, info\n",
        "\n",
        "    # Render the environment's current state\n",
        "    def render(self):\n",
        "        self.task_agent.render()\n",
        "\n",
        "    # Step function to perform an action and return the new state, reward, etc.\n",
        "    def step(self, action):\n",
        "        self.task_agent.perform_action(AgentAction(action))\n",
        "\n",
        "        # Calculate reward and termination condition\n",
        "        reward = 0\n",
        "        terminated = False\n",
        "        reward = int(self.task_agent.reward_prob) * self.task_agent.stake\n",
        "        self.task_agent.score += reward\n",
        "        self.task_agent.reset_stake()\n",
        "        self.task_agent.reset_trial()\n",
        "\n",
        "        # Check for termination condition\n",
        "        if self.task_agent.trial_num >= 200:\n",
        "            terminated = True\n",
        "\n",
        "        # Update observation\n",
        "        observation = self.task_agent.state + self.task_agent.state_part2 + [self.task_agent.stake] + [self.task_agent.score]\n",
        "        obs = np.array([observation], dtype=np.int64).reshape((self.input_size,))\n",
        "\n",
        "        info = {'number of trials': self.task_agent.trial_num,\n",
        "                'stake': self.task_agent.stake,\n",
        "                'winning_matrix': self.task_agent.winning_prob_matrix}\n",
        "\n",
        "        return obs, reward, terminated, False, info\n",
        "\n",
        "\n",
        "# Register this environment with Gymnasium for use with gym.make()\n",
        "register(\n",
        "      id='two-step-task-novel-v0',  # ID for the environment\n",
        "      entry_point=TwoStepTaskEnv,   # Class used to create the environment\n",
        ")\n"
      ],
      "metadata": {
        "id": "S5cyiD6i3DF6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3ef11b1-6769-4a74-e5f7-b05d7bf75428"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gymnasium/envs/registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment two-step-task-novel-v0 already in registry.\u001b[0m\n",
            "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2_Y-RHuve3Ys"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}